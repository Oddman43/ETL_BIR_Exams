{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "56vKIohe8Jou",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data Extraction\n",
    "This section details the process of extracting question data from a PDF file and corresponding answer keys from a TSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kRORmWM8Joy"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas for data manipulation\n",
    "* NumPy for the NaN datatype\n",
    "* PyPDF for its ability to read the pfd files, although it requieres post-processing of the extracted text\n",
    "* sqlite3 to create load the clean data into `bir_warehouse.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8DpYYll8Joz",
    "outputId": "40cac5b3-1153-431b-f780-201aa62b5813"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting year and type of exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `compute_expected_rows`, takes the exam year and topic as input and returns metadata crucial for subsequent data cleaning and loading steps. This metadata includes the expected maximum number of rows in the final DataFrame, the number of questions per exam, and the acronym for the exam type.  The maximum number of rows is calculated based on the number of questions and options per question for each year, following a specific pattern:\n",
    "\n",
    "*   2024-2021: 210 questions, 4 options each\n",
    "*   2020-2019: 185 questions, 4 options each\n",
    "*   2018-2015: 235 questions, 4 options each\n",
    "*   2014-2012: 235 questions, 5 options each\n",
    "*   2011-2004: 260 questions, 5 options each\n",
    "\n",
    "This information is essential for validating data integrity during the ETL process.  For example, the expected row count helps verify that all questions and options have been correctly extracted and transformed. The exam type acronym aids in consistent file naming and data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expected_rows(year: int, topic: str) -> tuple:\n",
    "    \"\"\"Computes and returns metadata about the exam for a given year and topic\n",
    "\n",
    "    Args:\n",
    "        year (int): The year of the exam\n",
    "        topic (str): The topic of the exam\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains:\n",
    "            - max_rows: The expected maximum number of rows in the DataFrame\n",
    "            - num_options: The number of questions in the exam\n",
    "            - exam_type_acronym: The acronym for the exam type\n",
    "\n",
    "    \"\"\"\n",
    "    info_year_dict: dict = {\n",
    "        2011: [260, 5],\n",
    "        2014: [235, 5],\n",
    "        2018: [235, 4],\n",
    "        2020: [185, 4],\n",
    "        2021: [210, 4],\n",
    "    }\n",
    "    max_rows: int = 0\n",
    "    num_options: int = 0\n",
    "    for target_year, info_list in info_year_dict.items():\n",
    "        if year <= target_year:\n",
    "            max_rows = (info_list[0] * info_list[1]) + info_list[0]\n",
    "            num_options = info_list[1]\n",
    "            break\n",
    "    else:\n",
    "        max_rows = 1050\n",
    "        num_options = 4\n",
    "    return max_rows, num_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add year of the exam and topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year: int = ...\n",
    "topic: str = \"BIOLOGÍA\"\n",
    "exam_acronym: str = \"bir\"\n",
    "total_num_rows: int\n",
    "num_questions: int\n",
    "total_num_rows, num_questions= compute_expected_rows(year, topic)\n",
    "save_format: str = f\"../data/clean/clean_{exam_acronym}_{year}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1Kjq5ue8Jo1"
   },
   "source": [
    "## Extracting Questions from Pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on extracting the textual content of the exam questions from the provided PDF file.\n",
    "\n",
    "**Path Definition:**\n",
    "\n",
    "The path to the PDF file is dynamically constructed using the provided `year` and `topic` variables. This ensures flexibility and allows the script to process different exam files without manual path adjustments.\n",
    "\n",
    "Finally it stores the output in a DataFrame and the first 10 rows of the resulting DataFrame are displayed for a quick preview of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB4M4yfr8Jo3"
   },
   "outputs": [],
   "source": [
    "path: str = f\"../data/raw/type_2/Raw_Cuaderno_{year}_{topic}_0_C.pdf\"\n",
    "with PdfReader(path) as pdf_file:\n",
    "    full_text: list = []\n",
    "    for n in range(2, len(pdf_file.pages)):\n",
    "        page = pdf_file.pages[n]\n",
    "        text: str = page.extract_text()\n",
    "        full_text.extend(text.splitlines())\n",
    "raw_exam_df: pd.DataFrame = pd.DataFrame(full_text, columns=[\"text\"])\n",
    "raw_exam_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBhc5aFQ8Jo6"
   },
   "source": [
    "## Extracting Answers from .tsv file into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on extracting the answer key data from the provided .tsv file and loading it into a Pandas DataFrame.\n",
    "\n",
    "**Path Definition and Data Loading:**\n",
    "\n",
    "The path to the .tsv file is dynamically constructed using the `year` and `topic` variables, mirroring the approach used for the PDF file. This ensures consistency and flexibility in processing different exam files. \n",
    "\n",
    "The `pd.read_table()` function is then used to read the .tsv file directly into a Pandas DataFrame. Finally, the first 10 rows of the resulting DataFrame are displayed for a quick preview of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "O1ouB9mQ8Jo7",
    "outputId": "70b7615d-9c90-4f2d-b9e1-a5c86b507ef6"
   },
   "outputs": [],
   "source": [
    "raw_answers_df: pd.DataFrame = pd.read_table(f\"../data/raw/type_2/Raw_Cuaderno_{year}_{topic}_0_C_Respuestas.txt\", sep= \" \", header=None)\n",
    "raw_answers_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFFlTADP8Jo7"
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing white space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QavIIOK8Jo7"
   },
   "source": [
    "This section focuses on cleaning the raw text extracted from the PDF exam file, preparing it for further processing. This involves removing extraneous whitespace, handling empty lines, and resetting the DataFrame index.\n",
    "\n",
    "**1. Removing Page Number Artifacts:**\n",
    "\n",
    "The extracted text may contain page number artifacts, which are removed by filtering out rows containing the string \"Página\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exam_df = raw_exam_df[~raw_exam_df[\"text\"].str.contains(\"Página\")]\n",
    "raw_exam_df = raw_exam_df[~raw_exam_df[\"text\"].str.contains(\"Pagina\")]\n",
    "\n",
    "footer_list: list = [f\"- {npag} -\" for npag in range(0,30,1)]\n",
    "\n",
    "for footer in footer_list:\n",
    "    raw_exam_df[\"text\"] = raw_exam_df[\"text\"].str.replace(footer, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Striping Leading/Trailing Whitespaces**\n",
    "\n",
    "Leading and trailing whitespace characters are removed from each text entry using the .str.strip() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exam_df = raw_exam_df[\"text\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Handling Empty Lines**\n",
    "\n",
    "Empty lines, represented as empty strings, are replaced with NaN (Not a Number) values. \n",
    "\n",
    "Then rows containing NaN values, representing empty lines, are removed from the DataFrame using .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exam_df = raw_exam_df.replace(\"\", np.nan)\n",
    "raw_exam_df = raw_exam_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Reseting DataFrame Index**\n",
    "\n",
    "The DataFrame index is reset after removing rows, ensuring a contiguous index and dropping the old index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exam_df = raw_exam_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Displays the first 10 rows of the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_exam_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Truncated Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section addresses the issue of truncated lines and multi-line questions or options within the extracted exam text. A custom function, `process_multi_line_str`, is used to concatenate these lines, ensuring that each question and option is presented as a single, coherent string.\n",
    "\n",
    "**Function: `process_multi_line_str(df)`**\n",
    "\n",
    "This function iterates through the DataFrame, performing two main tasks:\n",
    "\n",
    "1.  **Joining Truncated Words:**\n",
    "    * It checks if a line ends with a hyphen (\"-\"), indicating a word that has been truncated at the end of a line.\n",
    "    * If a line is truncated, it removes the hyphen and appends the content of the following line to the current line.\n",
    "    * The following line is then removed from the DataFrame, and the index is reset.\n",
    "\n",
    "2.  **Joining Multi-Line Questions/Options:**\n",
    "    * It checks if the first character of the next line can be converted to an integer. This is used as a heuristic to identify the start of a new question or option.\n",
    "    * If the next line does not start with an integer (i.e., it's a continuation of the current question or option), it is appended to the current line, separated by a space.\n",
    "    * The next line is then removed, and the index is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYZHjn_C8Jo8"
   },
   "outputs": [],
   "source": [
    "def process_multi_line_str(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Function to handle truncated lines\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Returns the DataFrame without truncated lines\n",
    "    \"\"\"\n",
    "    i: int = 0\n",
    "    while i < len(df) - 1:\n",
    "        if i < len(df) -1  and df.iloc[i].endswith(\"-\"):\n",
    "            df.iloc[i] = df.iloc[i][:-1] + df.iloc[i + 1]\n",
    "            df = df.drop(i + 1)\n",
    "            df = df.reset_index(drop=True)\n",
    "        else:\n",
    "            i += 1\n",
    "    n: int = 0\n",
    "    while n < len(df) -1:\n",
    "        if n + 1 < len(df):\n",
    "            try:\n",
    "                int(df.iloc[n + 1][0:1])\n",
    "                n += 1\n",
    "            except ValueError:\n",
    "                df.iloc[n] = df.iloc[n] + \" \" + df.iloc[n + 1]\n",
    "                df = df.drop(n +1)\n",
    "                df = df.reset_index(drop= True)\n",
    "        else:\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_multi_line_str function is applied to the cleaned raw_exam_df DataFrame, and the resulting DataFrame is stored in exam_df_concat. The first few rows of the concatenated DataFrame are displayed to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2v6dmQuM8_v1",
    "outputId": "215f5328-a43d-4bbb-f221-fba6fc10a5c1"
   },
   "outputs": [],
   "source": [
    "exam_df_concat: pd.DataFrame = process_multi_line_str(raw_exam_df)\n",
    "exam_df_concat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating and Correction Incorrect Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on validating the number of rows in the cleaned DataFrame against the expected number and correcting any rows that do not conform to the expected format.\n",
    "\n",
    "**1. Calculating Expected and Actual Incorrect Rows:**\n",
    "\n",
    "We calculate the expected number of incorrect rows by subtracting the total expected rows from the current number of rows in the DataFrame. We then identify the actual number of incorrect rows by counting the rows that do not end with a period (\".\"), a colon (\":\") or interrogation sign (\"?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_incorrect_expected: int = len(exam_df_concat) - total_num_rows\n",
    "num_rows_incorrect: int = exam_df_concat[~exam_df_concat.str.endswith((\".\", \":\", \"?\"))].count()\n",
    "id_rows_incorrect: list = exam_df_concat[~exam_df_concat.str.endswith((\".\", \":\", \"?\"))].index.to_list()\n",
    "print(f\"Number of expected incorrect rows = {num_rows_incorrect_expected}\")\n",
    "print(f\"Number of rows not ending with '.', ':' or '?' = {num_rows_incorrect}\")\n",
    "print(id_rows_incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Function: print_rows_incorrrect(wrong_id, df):**\n",
    "\n",
    "This function takes a list of incorrect row indices and the DataFrame as input. It then prints the row identified as incorrect along with the 2 rows before and 2 rows after, providing context to help determine how to fix the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rows_incorrrect(wrong_id: list[int], df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print +/- 2 rows for context\n",
    "\n",
    "    Args:\n",
    "        wrong_id (list): Generated list of wrong row_id\n",
    "        df (pd.DataFrame): DataFrame corresponding with the worng_id list\n",
    "    \"\"\"\n",
    "    for wid in wrong_id:\n",
    "        print(f\"Id to fix {wid}:\")\n",
    "        print(df.iloc[wid -2: wid +3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Function: fix_incorrect(num_row, df):**\n",
    "\n",
    "This function takes a list of row indices to fix and the DataFrame. It iterates through the indices in reverse order to avoid index shifting problems. Each incorrect row is concatenated with the row immediately following it, and the subsequent row is then dropped. The DataFrame index is reset after each concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_incorrect(num_row: list[int], df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fix the incorrect rows in the DataFrame\n",
    "\n",
    "    Args:\n",
    "        num_row (list): List of ids to fix\n",
    "        df (pd.DataFrame): The DataFrame to fix\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Fixed\n",
    "    \"\"\"\n",
    "    num_row: list = sorted(num_row, reverse=True)\n",
    "    for n in num_row:\n",
    "        df.iloc[n] = df.iloc[n] + df.iloc[n + 1]\n",
    "        df = df.drop(n + 1)\n",
    "        df = df.reset_index(drop= True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Validation and Correction Logic:**\n",
    "\n",
    "We compare the expected and actual number of incorrect rows. If they match, we proceed to fix the incorrect rows using the `fix_incorrect` function. Otherwise, we print the context of the incorrect rows and raise a warning, indicating that manual inspection and correction are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_rows_incorrect_expected == num_rows_incorrect:\n",
    "    exam_df_fixed = fix_incorrect(id_rows_incorrect, exam_df_concat)\n",
    "    df_correct_rows: bool = len(exam_df_fixed) == num_rows_incorrect_expected\n",
    "elif num_rows_incorrect == 0:\n",
    "    exam_df_fixed = exam_df_concat\n",
    "else:\n",
    "    print_rows_incorrrect(id_rows_incorrect, exam_df_concat)\n",
    "    raise Warning(\"More incorrect rows that expected, uncomment the lines below and add ids to the list, and comment this line\")\n",
    "    # rows_fix: list[int] = []\n",
    "    # exam_df_fixed: pd.DataFrame = fix_incorrect(rows_fix, exam_df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Verification of Corrected Rows:**\n",
    "\n",
    "We print the expected total number of rows and the actual number of rows in the corrected DataFrame to verify the results of the correction process.\n",
    "\n",
    "If the number is diferent it raises a Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correct number of rows = {total_num_rows}\\nNumber of rows in df = {exam_df_fixed.shape[0]}\")\n",
    "if total_num_rows != exam_df_fixed.shape[0]:\n",
    "    raise Warning(\"More or Less rows than expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting the DataFrame\n",
    "\n",
    "This section focuses on transforming the cleaned exam data into a structured format suitable for analysis. The DataFrame is pivoted to create columns for each question and its corresponding options.\n",
    "\n",
    "**1. Preparing the DataFrame for Pivoting:**\n",
    "\n",
    "The `exam_df_fixed` Series is converted to a DataFrame, and a 'group' column is created to identify each question and its options. An 'option_num' column is then generated to enumerate the options within each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df_fixed = exam_df_fixed.to_frame()\n",
    "groups: pd.Series = pd.Series((exam_df_fixed.index // (num_questions+1)) +1)\n",
    "exam_df_fixed[\"group\"] = groups  \n",
    "exam_df_fixed[\"option_num\"] = exam_df_fixed.groupby(\"group\").cumcount() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Pivoting the DataFrame:**\n",
    "\n",
    "The DataFrame is pivoted using the 'group' column as the index, the 'option_num' column as the columns, and the 'text' column as the values. The index is then reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df_pivot: pd.DataFrame = exam_df_fixed.pivot(index=\"group\", columns=\"option_num\", values=\"text\")\n",
    "exam_df_pivot = exam_df_pivot.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Renaming Columns:**\n",
    "\n",
    "The columns are renamed to more descriptive names, such as \"Question\", \"Option_1\", \"Option_2\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list: list = [x for x in range(1, num_questions+2, 1)]\n",
    "val_list: list = [\"Question\"] + [f\"Option_{x}\" for x in range(1, num_questions + 1, 1)]\n",
    "\n",
    "exam_df_pivot = exam_df_pivot.rename_axis(None, axis=1).rename(columns=dict(zip(key_list, val_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Dropping the 'group' Column:**\n",
    "\n",
    "The 'group' column is dropped as it is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df_pivot = exam_df_pivot.drop(columns=[\"group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Displaying the Pivoted DataFrame:**\n",
    "\n",
    "The first 10 rows of the pivoted DataFrame are displayed to verify the results of the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_df_pivot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers tsv\n",
    "\n",
    "This section focuses on cleaning and consolidating the answer key data from the TSV file. The original data contains duplicate columns, which are processed and combined into a single, clean DataFrame.\n",
    "\n",
    "**1. Initial Cleaning and Numeric Conversion:**\n",
    "\n",
    "* Iterates through all columns of the `raw_answers_df` DataFrame.\n",
    "* For string-type columns, replaces all occurrences of \"A\" with \"0\".\n",
    "* Non-string columns are ignored.\n",
    "* Applies `pd.to_numeric` to all columns, converting values to numbers (floats) and handling conversion errors with `errors='coerce'`, which replaces non-numeric values with `NaN`.\n",
    "* Replaces all `NaN` values with `999.9`.\n",
    "* Displays the first few rows of the transformed DataFrame (`raw_answers_df_2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in raw_answers_df:\n",
    "    try:\n",
    "        raw_answers_df[col] = raw_answers_df[col].str.replace(\"A\", \"0\")\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "raw_answers_df_2 = raw_answers_df.apply(pd.to_numeric, errors= 'coerce')\n",
    "raw_answers_df_2 = raw_answers_df_2.fillna(999.9)\n",
    "raw_answers_df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answers_df_2.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Transformation to \"Longer\" Format:**\n",
    "\n",
    "* Creates two empty lists, number_question_answers and number_question_correct.\n",
    "* Iterates through the rows of raw_answers_df_2 in steps of two.\n",
    "* For each pair of rows, the values from the first row are appended to the number_question_answers list, and the values from the second row are appended to the number_question_correct list.\n",
    "* Creates a new DataFrame raw_answers_df_longer with the number_question_answers and number_question_correct lists as columns \"V\" and \"RC\", respectively.\n",
    "* Displays the first few rows of raw_answers_df_longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_question_answers: list = []\n",
    "number_question_correct: list = []\n",
    "\n",
    "for i in range(0, len(raw_answers_df_2), 2):\n",
    "    number_question_answers.extend(raw_answers_df_2.iloc[i].tolist())\n",
    "    number_question_correct.extend(raw_answers_df_2.iloc[i + 1].tolist())\n",
    "\n",
    "raw_answers_df_longer: pd.DataFrame = pd.DataFrame({\"V\": number_question_answers, \"RC\": number_question_correct})\n",
    "raw_answers_df_longer.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Final Sorting and Cleaning:**\n",
    "\n",
    "* Sorts the raw_answers_df_longer DataFrame by the \"V\" column.\n",
    "* Replaces all 999.9 values with NaN.\n",
    "* Removes all rows containing NaN.\n",
    "* Displays the first few rows of the cleaned DataFrame (answers_df_clean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answers_df_sorted = raw_answers_df_longer.sort_values(by=\"V\")\n",
    "raw_answers_df_sorted = raw_answers_df_sorted.replace(999.9, np.nan)\n",
    "answers_df_clean = raw_answers_df_sorted.dropna()\n",
    "answers_df_clean = answers_df_clean.reset_index(drop=True)\n",
    "answers_df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df_clean.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the DataFrames\n",
    "\n",
    "This section focuses on merging the processed exam questions DataFrame with the cleaned answers DataFrame, performing final data cleaning, and verifying the integrity of the joined data.\n",
    "\n",
    "**1. Joining the DataFrames:**\n",
    "\n",
    "The pivoted exam questions DataFrame (`exam_df_pivot`) and the cleaned answers DataFrame (`answers_df_clean`) are joined horizontally using `pd.concat`. The 'V0' column, which represents the question number from the answers DataFrame, is then dropped. The first 10 rows of the joined DataFrame are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df: pd.DataFrame = pd.concat([exam_df_pivot, answers_df_clean], axis=1)\n",
    "processed_exam_df = processed_exam_df.drop(columns=[\"V\"])\n",
    "processed_exam_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Identifying and Handling Missing Values:**\n",
    "\n",
    "Missing values are then replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df = processed_exam_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Verifying Missing Values and Data Types:**\n",
    "\n",
    "The number of missing values in each column is checked, and the data types of the columns are displayed. The 'RC' (correct answer) column is then converted to integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df.isnull().sum()\n",
    "processed_exam_df[\"RC\"] = processed_exam_df[\"RC\"].astype(int)\n",
    "processed_exam_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Adding year column**\n",
    "\n",
    "A column containing the year of the exam is added to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df[\"year\"] = year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Displaying the Final DataFrame:**\n",
    "\n",
    "The first 10 rows of the final joined and cleaned DataFrame are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving in csv file\n",
    "\n",
    "The `processed_exam_df` DataFrame, containing the cleaned and transformed exam data, is saved to a CSV file with the name generatd in the first step in the \"data\" directory. The `index=False` argument ensures that the DataFrame index is not written to the file, resulting in a cleaner output. This CSV file can be used for further analysis or as input for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exam_df.to_csv(save_format, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing into data warehouse\n",
    "\n",
    "This section details the process of loading the cleaned and transformed exam data into an SQLite database, serving as a data warehouse for further analysis and querying.\n",
    "\n",
    "**1. Database Connection and Path:**\n",
    "\n",
    "The path to the SQLite database file is defined, and a connection is established using the `sqlite3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"../data/clean/bir_warehouse.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. SQL Queries Definition:**\n",
    "\n",
    "Two SQL queries are defined: one for inserting questions into the questions table and another for inserting options into the questions_options table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_question: str = \"\"\"\n",
    "INSERT INTO questions (exam_year, exam_subject, question) \n",
    "VALUES((SELECT id_year FROM year WHERE year_name = ?),\n",
    "    (SELECT id_type FROM exam WHERE exam_type = ?),\n",
    "    ?);\n",
    "\"\"\"\n",
    "\n",
    "query_options: str = \"\"\"\n",
    "INSERT INTO questions_options (question_id, option_num, option_text, is_correct)\n",
    "VALUES((SELECT q.id \n",
    "        FROM questions AS q\n",
    "        JOIN year AS y ON q.exam_year = y.id_year\n",
    "        WHERE q.question = ? AND y.year_name = ?),\n",
    "    ?,\n",
    "    ?,\n",
    "    ?)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Database Interaction and Data Insertion:**\n",
    "\n",
    "A with statement is used to establish a connection to the SQLite database, ensuring that the connection is properly closed after use. A cursor is created, and the processed_exam_df DataFrame is iterated over to insert each question and its options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_path) as bir_warehouse:\n",
    "    cur = bir_warehouse.cursor()\n",
    "    for question in processed_exam_df.itertuples():\n",
    "        cur.execute(query_question, (str(question[7]), exam_acronym, question[1]))\n",
    "        bir_warehouse.commit()\n",
    "        for n, option in enumerate(question[2:6]):\n",
    "            cur.execute(query_options, (question[1], str(year),n+1, option, n+1 == question[6]))\n",
    "            bir_warehouse.commit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bir_etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
